{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3435ff40-0df0-45e3-89da-bd3beab2f689",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Mini Project - AZ5411 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708057d0-4ddc-4a01-8363-a7f78a85ce0b",
   "metadata": {},
   "source": [
    "## AI Powered Alchemy - Poorna prakash S, Srilakshmi H and Thanya Gayathri N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ea3a6-339b-4ec8-b34e-8b6bdfb96b87",
   "metadata": {},
   "source": [
    "## Review 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9b1a2",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2348e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MolecularFormula</th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>InChI</th>\n",
       "      <th>InChIKey</th>\n",
       "      <th>IUPACName</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>MonoisotopicMass</th>\n",
       "      <th>...</th>\n",
       "      <th>FeatureAcceptorCount3D</th>\n",
       "      <th>FeatureDonorCount3D</th>\n",
       "      <th>FeatureAnionCount3D</th>\n",
       "      <th>FeatureCationCount3D</th>\n",
       "      <th>FeatureRingCount3D</th>\n",
       "      <th>FeatureHydrophobeCount3D</th>\n",
       "      <th>ConformerModelRMSD3D</th>\n",
       "      <th>EffectiveRotorCount3D</th>\n",
       "      <th>ConformerCount3D</th>\n",
       "      <th>pIC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2744814</td>\n",
       "      <td>ClC1=CC(NC(=O)CSC2=NC=CC(=N2)C2=CSC(=N2)C2=CC=...</td>\n",
       "      <td>C21H14Cl2N4OS2</td>\n",
       "      <td>473.4</td>\n",
       "      <td>InChI=1S/C21H14Cl2N4OS2/c22-14-8-15(23)10-16(9...</td>\n",
       "      <td>LILOEJREEQFTPM-UHFFFAOYSA-N</td>\n",
       "      <td>N-(3,5-dichlorophenyl)-2-[4-(2-phenyl-1,3-thia...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>471.998609</td>\n",
       "      <td>471.998609</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.477121255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2821293</td>\n",
       "      <td>CN1N=C(C=C1C(F)(F)F)C1=CC=C(S1)C1=CC=NC(SCC(=O...</td>\n",
       "      <td>C21H15ClF3N5OS2</td>\n",
       "      <td>510.0</td>\n",
       "      <td>InChI=1S/C21H15ClF3N5OS2/c1-30-18(21(23,24)25)...</td>\n",
       "      <td>AWQBIBTZJKFLEW-UHFFFAOYSA-N</td>\n",
       "      <td>N-(4-chlorophenyl)-2-[4-[5-[1-methyl-5-(triflu...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>509.035865</td>\n",
       "      <td>509.035865</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2820912</td>\n",
       "      <td>CSC1=C(C(C)=C(S1)C1=NC(C)=CS1)C1=CC=NC(SCC(=O)...</td>\n",
       "      <td>C22H19ClN4OS4</td>\n",
       "      <td>519.1</td>\n",
       "      <td>InChI=1S/C22H19ClN4OS4/c1-12-10-30-20(25-12)19...</td>\n",
       "      <td>WRXXISITJDZVCL-UHFFFAOYSA-N</td>\n",
       "      <td>N-(4-chlorophenyl)-2-[4-[4-methyl-2-methylsulf...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>518.013024</td>\n",
       "      <td>518.013024</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.041392685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2820914</td>\n",
       "      <td>CSC1=C(C(C)=C(S1)C1=NC(C)=CS1)C1=CC=NC(SCC(=O)...</td>\n",
       "      <td>C22H19ClN4OS4</td>\n",
       "      <td>519.1</td>\n",
       "      <td>InChI=1S/C22H19ClN4OS4/c1-12-10-30-20(25-12)19...</td>\n",
       "      <td>NNVVKOVHRSDRSQ-UHFFFAOYSA-N</td>\n",
       "      <td>N-(2-chlorophenyl)-2-[4-[4-methyl-2-methylsulf...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>518.013024</td>\n",
       "      <td>518.013024</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BLINDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2744846</td>\n",
       "      <td>CC1=NC(=CS1)C1=NC(=CS1)C1=NC(SCC(=O)NC2=CC=C(C...</td>\n",
       "      <td>C19H14ClN5OS3</td>\n",
       "      <td>460.0</td>\n",
       "      <td>InChI=1S/C19H14ClN5OS3/c1-11-22-16(9-27-11)18-...</td>\n",
       "      <td>JEZYTEDGOJCVQS-UHFFFAOYSA-N</td>\n",
       "      <td>N-(4-chlorophenyl)-2-[4-[2-(2-methyl-1,3-thiaz...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>459.004901</td>\n",
       "      <td>459.004901</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.146128036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CID                                             SMILES  \\\n",
       "0  2744814  ClC1=CC(NC(=O)CSC2=NC=CC(=N2)C2=CSC(=N2)C2=CC=...   \n",
       "1  2821293  CN1N=C(C=C1C(F)(F)F)C1=CC=C(S1)C1=CC=NC(SCC(=O...   \n",
       "2  2820912  CSC1=C(C(C)=C(S1)C1=NC(C)=CS1)C1=CC=NC(SCC(=O)...   \n",
       "3  2820914  CSC1=C(C(C)=C(S1)C1=NC(C)=CS1)C1=CC=NC(SCC(=O)...   \n",
       "4  2744846  CC1=NC(=CS1)C1=NC(=CS1)C1=NC(SCC(=O)NC2=CC=C(C...   \n",
       "\n",
       "  MolecularFormula  MolecularWeight  \\\n",
       "0   C21H14Cl2N4OS2            473.4   \n",
       "1  C21H15ClF3N5OS2            510.0   \n",
       "2    C22H19ClN4OS4            519.1   \n",
       "3    C22H19ClN4OS4            519.1   \n",
       "4    C19H14ClN5OS3            460.0   \n",
       "\n",
       "                                               InChI  \\\n",
       "0  InChI=1S/C21H14Cl2N4OS2/c22-14-8-15(23)10-16(9...   \n",
       "1  InChI=1S/C21H15ClF3N5OS2/c1-30-18(21(23,24)25)...   \n",
       "2  InChI=1S/C22H19ClN4OS4/c1-12-10-30-20(25-12)19...   \n",
       "3  InChI=1S/C22H19ClN4OS4/c1-12-10-30-20(25-12)19...   \n",
       "4  InChI=1S/C19H14ClN5OS3/c1-11-22-16(9-27-11)18-...   \n",
       "\n",
       "                      InChIKey  \\\n",
       "0  LILOEJREEQFTPM-UHFFFAOYSA-N   \n",
       "1  AWQBIBTZJKFLEW-UHFFFAOYSA-N   \n",
       "2  WRXXISITJDZVCL-UHFFFAOYSA-N   \n",
       "3  NNVVKOVHRSDRSQ-UHFFFAOYSA-N   \n",
       "4  JEZYTEDGOJCVQS-UHFFFAOYSA-N   \n",
       "\n",
       "                                           IUPACName  XLogP   ExactMass  \\\n",
       "0  N-(3,5-dichlorophenyl)-2-[4-(2-phenyl-1,3-thia...    5.6  471.998609   \n",
       "1  N-(4-chlorophenyl)-2-[4-[5-[1-methyl-5-(triflu...    4.9  509.035865   \n",
       "2  N-(4-chlorophenyl)-2-[4-[4-methyl-2-methylsulf...    6.3  518.013024   \n",
       "3  N-(2-chlorophenyl)-2-[4-[4-methyl-2-methylsulf...    6.3  518.013024   \n",
       "4  N-(4-chlorophenyl)-2-[4-[2-(2-methyl-1,3-thiaz...    4.4  459.004901   \n",
       "\n",
       "   MonoisotopicMass  ...  FeatureAcceptorCount3D  FeatureDonorCount3D  \\\n",
       "0        471.998609  ...                     3.0                  1.0   \n",
       "1        509.035865  ...                     3.0                  1.0   \n",
       "2        518.013024  ...                     3.0                  1.0   \n",
       "3        518.013024  ...                     3.0                  1.0   \n",
       "4        459.004901  ...                     4.0                  1.0   \n",
       "\n",
       "   FeatureAnionCount3D  FeatureCationCount3D  FeatureRingCount3D  \\\n",
       "0                  0.0                   1.0                 4.0   \n",
       "1                  0.0                   1.0                 4.0   \n",
       "2                  0.0                   1.0                 4.0   \n",
       "3                  0.0                   1.0                 4.0   \n",
       "4                  0.0                   1.0                 4.0   \n",
       "\n",
       "   FeatureHydrophobeCount3D  ConformerModelRMSD3D  EffectiveRotorCount3D  \\\n",
       "0                       0.0                   1.0                    7.0   \n",
       "1                       0.0                   1.2                    8.0   \n",
       "2                       1.0                   1.0                    8.0   \n",
       "3                       1.0                   1.2                    8.0   \n",
       "4                       0.0                   1.0                    7.0   \n",
       "\n",
       "   ConformerCount3D         pIC50  \n",
       "0              10.0  -0.477121255  \n",
       "1              10.0            -1  \n",
       "2              10.0  -1.041392685  \n",
       "3              10.0       BLINDED  \n",
       "4              10.0  -1.146128036  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv('DDH Data with Properties.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1254ae",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c294a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CID                         0\n",
       "SMILES                      0\n",
       "MolecularFormula            0\n",
       "MolecularWeight             0\n",
       "InChI                       0\n",
       "InChIKey                    3\n",
       "IUPACName                   3\n",
       "XLogP                       3\n",
       "ExactMass                   3\n",
       "MonoisotopicMass            3\n",
       "TPSA                        3\n",
       "Complexity                  3\n",
       "Charge                      3\n",
       "HBondDonorCount             3\n",
       "HBondAcceptorCount          3\n",
       "RotatableBondCount          3\n",
       "HeavyAtomCount              3\n",
       "IsotopeAtomCount            3\n",
       "AtomStereoCount             3\n",
       "DefinedAtomStereoCount      3\n",
       "UndefinedAtomStereoCount    3\n",
       "BondStereoCount             3\n",
       "DefinedBondStereoCount      3\n",
       "UndefinedBondStereoCount    3\n",
       "CovalentUnitCount           3\n",
       "Volume3D                    3\n",
       "XStericQuadrupole3D         4\n",
       "YStericQuadrupole3D         4\n",
       "ZStericQuadrupole3D         4\n",
       "FeatureCount3D              4\n",
       "FeatureAcceptorCount3D      4\n",
       "FeatureDonorCount3D         4\n",
       "FeatureAnionCount3D         4\n",
       "FeatureCationCount3D        4\n",
       "FeatureRingCount3D          4\n",
       "FeatureHydrophobeCount3D    4\n",
       "ConformerModelRMSD3D        4\n",
       "EffectiveRotorCount3D       4\n",
       "ConformerCount3D            4\n",
       "pIC50                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed0c86",
   "metadata": {},
   "source": [
    "## Check for the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6360c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408e5eb",
   "metadata": {},
   "source": [
    "## Variational Auto Encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802c440-ebef-441d-b452-5cd2d4b8d16f",
   "metadata": {},
   "source": [
    "### 1. Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "281d5145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 89.99439591627855\n",
      "Epoch 2, Loss: 66.51373202984149\n",
      "Epoch 3, Loss: 59.51921404325045\n",
      "Epoch 4, Loss: 53.77498391958383\n",
      "Epoch 5, Loss: 51.96206606351412\n",
      "Epoch 6, Loss: 48.92057624230018\n",
      "Epoch 7, Loss: 47.929218585674576\n",
      "Epoch 8, Loss: 45.62708106407752\n",
      "Epoch 9, Loss: 44.706348125751205\n",
      "Epoch 10, Loss: 43.84571192814754\n",
      "Epoch 11, Loss: 42.738507784329926\n",
      "Epoch 12, Loss: 42.22827559251051\n",
      "Epoch 13, Loss: 41.19544865534856\n",
      "Epoch 14, Loss: 40.622418036827675\n",
      "Epoch 15, Loss: 40.20584957416241\n",
      "Epoch 16, Loss: 39.580386235163765\n",
      "Epoch 17, Loss: 39.15222402719351\n",
      "Epoch 18, Loss: 38.14267202524039\n",
      "Epoch 19, Loss: 37.891137049748345\n",
      "Epoch 20, Loss: 37.27158473088191\n",
      "Epoch 21, Loss: 36.5541017972506\n",
      "Epoch 22, Loss: 36.392272068904\n",
      "Epoch 23, Loss: 35.606068244347206\n",
      "Epoch 24, Loss: 35.44554959810697\n",
      "Epoch 25, Loss: 34.318899594820465\n",
      "Epoch 26, Loss: 34.40858840942383\n",
      "Epoch 27, Loss: 34.3684076162485\n",
      "Epoch 28, Loss: 33.923967801607574\n",
      "Epoch 29, Loss: 33.63360830453726\n",
      "Epoch 30, Loss: 34.17352426969088\n",
      "Epoch 31, Loss: 32.5423698425293\n",
      "Epoch 32, Loss: 32.73903949444111\n",
      "Epoch 33, Loss: 32.56387710571289\n",
      "Epoch 34, Loss: 32.25309225229117\n",
      "Epoch 35, Loss: 32.84747490516076\n",
      "Epoch 36, Loss: 31.92230928861178\n",
      "Epoch 37, Loss: 31.321029076209435\n",
      "Epoch 38, Loss: 31.330801303570087\n",
      "Epoch 39, Loss: 30.76685670705942\n",
      "Epoch 40, Loss: 30.853921450101414\n",
      "Epoch 41, Loss: 30.143162213839016\n",
      "Epoch 42, Loss: 30.207924182598408\n",
      "Epoch 43, Loss: 29.59936229999249\n",
      "Epoch 44, Loss: 29.53244825509878\n",
      "Epoch 45, Loss: 30.188901754525993\n",
      "Epoch 46, Loss: 29.708480248084435\n",
      "Epoch 47, Loss: 29.715538024902344\n",
      "Epoch 48, Loss: 29.1734011723445\n",
      "Epoch 49, Loss: 29.10383077768179\n",
      "Epoch 50, Loss: 28.95418225801908\n",
      "Epoch 51, Loss: 29.75656171945425\n",
      "Epoch 52, Loss: 29.084144005408653\n",
      "Epoch 53, Loss: 28.895765891441933\n",
      "Epoch 54, Loss: 28.42566094031701\n",
      "Epoch 55, Loss: 28.46595265315129\n",
      "Epoch 56, Loss: 27.837408505953274\n",
      "Epoch 57, Loss: 28.95440541780912\n",
      "Epoch 58, Loss: 28.37183350783128\n",
      "Epoch 59, Loss: 28.765305592463566\n",
      "Epoch 60, Loss: 28.631849435659554\n",
      "Epoch 61, Loss: 28.179337428166317\n",
      "Epoch 62, Loss: 28.21053123474121\n",
      "Epoch 63, Loss: 27.74185899587778\n",
      "Epoch 64, Loss: 27.356926697951096\n",
      "Epoch 65, Loss: 27.412043791550857\n",
      "Epoch 66, Loss: 27.386300013615536\n",
      "Epoch 67, Loss: 27.555234762338493\n",
      "Epoch 68, Loss: 27.26676955589881\n",
      "Epoch 69, Loss: 27.00646209716797\n",
      "Epoch 70, Loss: 27.56237088716947\n",
      "Epoch 71, Loss: 26.968705103947567\n",
      "Epoch 72, Loss: 27.54881580059345\n",
      "Epoch 73, Loss: 27.25672619159405\n",
      "Epoch 74, Loss: 26.652905684251053\n",
      "Epoch 75, Loss: 27.384228926438553\n",
      "Epoch 76, Loss: 27.045532813439003\n",
      "Epoch 77, Loss: 26.441226665790264\n",
      "Epoch 78, Loss: 26.94044934786283\n",
      "Epoch 79, Loss: 26.470939782949593\n",
      "Epoch 80, Loss: 27.327013162466194\n",
      "Epoch 81, Loss: 26.6382328913762\n",
      "Epoch 82, Loss: 26.61282436664288\n",
      "Epoch 83, Loss: 26.360201908991886\n",
      "Epoch 84, Loss: 26.81129088768592\n",
      "Epoch 85, Loss: 26.647683216975285\n",
      "Epoch 86, Loss: 26.645666709313026\n",
      "Epoch 87, Loss: 26.294306388268105\n",
      "Epoch 88, Loss: 25.919505192683292\n",
      "Epoch 89, Loss: 26.60513188288762\n",
      "Epoch 90, Loss: 25.88956510103666\n",
      "Epoch 91, Loss: 26.884812428401066\n",
      "Epoch 92, Loss: 26.030449647169846\n",
      "Epoch 93, Loss: 26.332992260272686\n",
      "Epoch 94, Loss: 26.311600024883564\n",
      "Epoch 95, Loss: 26.81517791748047\n",
      "Epoch 96, Loss: 25.410815165593075\n",
      "Epoch 97, Loss: 25.971483964186447\n",
      "Epoch 98, Loss: 25.214795919565056\n",
      "Epoch 99, Loss: 26.147003760704628\n",
      "Epoch 100, Loss: 25.899748141948994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Define a custom dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list):\n",
    "        self.smiles_list = smiles_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles_list[idx]\n",
    "\n",
    "# Define the VAE architecture for SMILES\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size * 2)  # *2 for mean and log-variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu = mu_logvar[:, :self.latent_size]\n",
    "        logvar = mu_logvar[:, self.latent_size:]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Define training function\n",
    "def train_vae(model, dataloader, optimizer, loss_function, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for smiles in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = smiles_to_fingerprint(smiles)\n",
    "            inputs = inputs.to(device)\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon_batch, inputs, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Function to convert SMILES to Morgan fingerprint\n",
    "def smiles_to_fingerprint(smiles_list, max_length=100):\n",
    "    fingerprint_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            fingerprint_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_fingerprint = nn.utils.rnn.pad_sequence(fingerprint_list, batch_first=True, padding_value=0)\n",
    "    return padded_fingerprint\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "# Define hyperparameters\n",
    "input_size = 1024  # Size of input fingerprint vector\n",
    "hidden_size = 256  # Size of hidden layer\n",
    "latent_size = 64   # Size of latent space\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize VAE model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae_model = VAE(input_size, hidden_size, latent_size).to(device)\n",
    "\n",
    "# Define optimizer with Adam\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the VAE\n",
    "train_vae(vae_model, dataloader, optimizer, loss_function, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(vae_model.state_dict(), 'vae_model_adam.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e18ae-6fbd-4e25-9d1d-7d260bb20c58",
   "metadata": {},
   "source": [
    "### 2. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85804879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 73.73299584021935\n",
      "Epoch 2, Loss: 50.53976499117338\n",
      "Epoch 3, Loss: 46.898092416616585\n",
      "Epoch 4, Loss: 43.96805631197416\n",
      "Epoch 5, Loss: 42.690128326416016\n",
      "Epoch 6, Loss: 41.14579567542443\n",
      "Epoch 7, Loss: 40.138592353233925\n",
      "Epoch 8, Loss: 39.99520580585186\n",
      "Epoch 9, Loss: 39.422271728515625\n",
      "Epoch 10, Loss: 38.44592461219201\n",
      "Epoch 11, Loss: 37.68438192514273\n",
      "Epoch 12, Loss: 37.981222299429085\n",
      "Epoch 13, Loss: 36.83351575411283\n",
      "Epoch 14, Loss: 36.64184218186598\n",
      "Epoch 15, Loss: 36.16028565626878\n",
      "Epoch 16, Loss: 36.030009049635666\n",
      "Epoch 17, Loss: 35.89184981126051\n",
      "Epoch 18, Loss: 34.63858883197491\n",
      "Epoch 19, Loss: 34.60590274517353\n",
      "Epoch 20, Loss: 34.38506874671349\n",
      "Epoch 21, Loss: 33.463575509878304\n",
      "Epoch 22, Loss: 33.31795853834886\n",
      "Epoch 23, Loss: 32.2475826556866\n",
      "Epoch 24, Loss: 33.00807101909931\n",
      "Epoch 25, Loss: 32.40277231656588\n",
      "Epoch 26, Loss: 31.601198343130257\n",
      "Epoch 27, Loss: 31.721334604116585\n",
      "Epoch 28, Loss: 32.08789297250601\n",
      "Epoch 29, Loss: 31.97854995727539\n",
      "Epoch 30, Loss: 31.114056807297928\n",
      "Epoch 31, Loss: 31.399618002084587\n",
      "Epoch 32, Loss: 30.918226975661057\n",
      "Epoch 33, Loss: 31.403782331026516\n",
      "Epoch 34, Loss: 31.10966550386869\n",
      "Epoch 35, Loss: 31.76034457866962\n",
      "Epoch 36, Loss: 30.72985869187575\n",
      "Epoch 37, Loss: 31.40815412081205\n",
      "Epoch 38, Loss: 30.93786386343149\n",
      "Epoch 39, Loss: 30.423185641948994\n",
      "Epoch 40, Loss: 29.842096475454476\n",
      "Epoch 41, Loss: 29.72820017887996\n",
      "Epoch 42, Loss: 29.929740025446964\n",
      "Epoch 43, Loss: 29.394530663123497\n",
      "Epoch 44, Loss: 29.59890629695012\n",
      "Epoch 45, Loss: 29.69218034010667\n",
      "Epoch 46, Loss: 29.4454953120305\n",
      "Epoch 47, Loss: 29.348612711979793\n",
      "Epoch 48, Loss: 29.32721856924204\n",
      "Epoch 49, Loss: 29.26080894470215\n",
      "Epoch 50, Loss: 29.128615599412186\n",
      "Epoch 51, Loss: 29.667834502000076\n",
      "Epoch 52, Loss: 29.04154161306528\n",
      "Epoch 53, Loss: 28.952762603759766\n",
      "Epoch 54, Loss: 28.86866862957294\n",
      "Epoch 55, Loss: 28.748032643244816\n",
      "Epoch 56, Loss: 29.851919027475212\n",
      "Epoch 57, Loss: 29.78561518742488\n",
      "Epoch 58, Loss: 28.84326934814453\n",
      "Epoch 59, Loss: 28.78365003145658\n",
      "Epoch 60, Loss: 29.54064002403846\n",
      "Epoch 61, Loss: 28.898761455829327\n",
      "Epoch 62, Loss: 28.674415001502403\n",
      "Epoch 63, Loss: 28.692459253164436\n",
      "Epoch 64, Loss: 28.324470960176907\n",
      "Epoch 65, Loss: 28.294253275944637\n",
      "Epoch 66, Loss: 28.276482362013596\n",
      "Epoch 67, Loss: 28.159460214468147\n",
      "Epoch 68, Loss: 28.469501201923077\n",
      "Epoch 69, Loss: 28.32238637484037\n",
      "Epoch 70, Loss: 28.540599676278923\n",
      "Epoch 71, Loss: 28.47383191035344\n",
      "Epoch 72, Loss: 27.975876734806942\n",
      "Epoch 73, Loss: 27.721898739154522\n",
      "Epoch 74, Loss: 27.777298267071064\n",
      "Epoch 75, Loss: 28.48413056593675\n",
      "Epoch 76, Loss: 27.820031532874474\n",
      "Epoch 77, Loss: 27.928145041832558\n",
      "Epoch 78, Loss: 27.490259170532227\n",
      "Epoch 79, Loss: 28.03757828932542\n",
      "Epoch 80, Loss: 27.321939028226414\n",
      "Epoch 81, Loss: 27.009830474853516\n",
      "Epoch 82, Loss: 27.234773489145134\n",
      "Epoch 83, Loss: 27.061311575082634\n",
      "Epoch 84, Loss: 27.339003342848557\n",
      "Epoch 85, Loss: 26.383300341092625\n",
      "Epoch 86, Loss: 26.41161199716421\n",
      "Epoch 87, Loss: 27.35538541353666\n",
      "Epoch 88, Loss: 26.754255001361553\n",
      "Epoch 89, Loss: 26.99850375835712\n",
      "Epoch 90, Loss: 26.54280060988206\n",
      "Epoch 91, Loss: 26.701584742619442\n",
      "Epoch 92, Loss: 26.516774104191708\n",
      "Epoch 93, Loss: 26.49584564795861\n",
      "Epoch 94, Loss: 26.75824268047626\n",
      "Epoch 95, Loss: 27.12147698035607\n",
      "Epoch 96, Loss: 27.320608872633713\n",
      "Epoch 97, Loss: 26.000367678128757\n",
      "Epoch 98, Loss: 26.175180581899788\n",
      "Epoch 99, Loss: 25.959337968092697\n",
      "Epoch 100, Loss: 26.171296779926006\n"
     ]
    }
   ],
   "source": [
    "# Define the VAE architecture for SMILES\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size * 2)  # *2 for mean and log-variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu = mu_logvar[:, :self.latent_size]\n",
    "        logvar = mu_logvar[:, self.latent_size:]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Define training function\n",
    "def train_vae(model, dataloader, optimizer, loss_function, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for smiles in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = smiles_to_fingerprint(smiles)\n",
    "            inputs = inputs.to(device)\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon_batch, inputs, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Function to convert SMILES to Morgan fingerprint\n",
    "def smiles_to_fingerprint(smiles_list, max_length=100):\n",
    "    fingerprint_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            fingerprint_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_fingerprint = nn.utils.rnn.pad_sequence(fingerprint_list, batch_first=True, padding_value=0)\n",
    "    return padded_fingerprint\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "# Define hyperparameters\n",
    "input_size = 1024  # Size of input fingerprint vector\n",
    "hidden_size = 256  # Size of hidden layer\n",
    "latent_size = 64   # Size of latent space\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5==0\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize VAE model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae_model = VAE(input_size, hidden_size, latent_size).to(device)\n",
    "\n",
    "# Define optimizer with SGD\n",
    "optimizer = optim.SGD(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the VAE\n",
    "train_vae(vae_model, dataloader, optimizer, loss_function, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(vae_model.state_dict(), 'vae_model_sgd.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e0614-70fe-4cee-af91-b3a686df8fa4",
   "metadata": {},
   "source": [
    "### 3. Root Mean Squared Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5a105a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 104.00414012028621\n",
      "Epoch 2, Loss: 52.071839552659256\n",
      "Epoch 3, Loss: 47.26261285635141\n",
      "Epoch 4, Loss: 45.93827643761268\n",
      "Epoch 5, Loss: 44.47152944711539\n",
      "Epoch 6, Loss: 44.01928124060998\n",
      "Epoch 7, Loss: 42.67973239605244\n",
      "Epoch 8, Loss: 41.3401853121244\n",
      "Epoch 9, Loss: 40.63094681959886\n",
      "Epoch 10, Loss: 40.0162972670335\n",
      "Epoch 11, Loss: 38.904425400954025\n",
      "Epoch 12, Loss: 38.09453993577223\n",
      "Epoch 13, Loss: 37.53650635939378\n",
      "Epoch 14, Loss: 36.67780421330379\n",
      "Epoch 15, Loss: 36.05838834322416\n",
      "Epoch 16, Loss: 35.88659902719351\n",
      "Epoch 17, Loss: 35.04010537954477\n",
      "Epoch 18, Loss: 34.92524983332707\n",
      "Epoch 19, Loss: 34.164138500507065\n",
      "Epoch 20, Loss: 33.96165422292856\n",
      "Epoch 21, Loss: 33.98830824631911\n",
      "Epoch 22, Loss: 34.408714294433594\n",
      "Epoch 23, Loss: 33.626050802377556\n",
      "Epoch 24, Loss: 32.84825926560622\n",
      "Epoch 25, Loss: 33.16204100388747\n",
      "Epoch 26, Loss: 33.17982248159555\n",
      "Epoch 27, Loss: 32.69991713303786\n",
      "Epoch 28, Loss: 32.66929582449106\n",
      "Epoch 29, Loss: 32.236294966477615\n",
      "Epoch 30, Loss: 32.24635021503155\n",
      "Epoch 31, Loss: 31.34200360224797\n",
      "Epoch 32, Loss: 31.313181070181038\n",
      "Epoch 33, Loss: 31.119365692138672\n",
      "Epoch 34, Loss: 30.712628584641678\n",
      "Epoch 35, Loss: 29.77448976956881\n",
      "Epoch 36, Loss: 30.40944055410532\n",
      "Epoch 37, Loss: 30.267600719745342\n",
      "Epoch 38, Loss: 29.844024364764874\n",
      "Epoch 39, Loss: 29.320889693040115\n",
      "Epoch 40, Loss: 30.435830923227165\n",
      "Epoch 41, Loss: 28.667095037607048\n",
      "Epoch 42, Loss: 29.528650723970852\n",
      "Epoch 43, Loss: 28.93161392211914\n",
      "Epoch 44, Loss: 29.17933596097506\n",
      "Epoch 45, Loss: 28.850848271296574\n",
      "Epoch 46, Loss: 28.562021402212288\n",
      "Epoch 47, Loss: 27.957694567166843\n",
      "Epoch 48, Loss: 28.554731369018555\n",
      "Epoch 49, Loss: 28.118910569411057\n",
      "Epoch 50, Loss: 28.295110262357273\n",
      "Epoch 51, Loss: 27.714992376474235\n",
      "Epoch 52, Loss: 28.294090417715218\n",
      "Epoch 53, Loss: 28.89026010953463\n",
      "Epoch 54, Loss: 28.104501870962288\n",
      "Epoch 55, Loss: 27.929775824913612\n",
      "Epoch 56, Loss: 27.24481934767503\n",
      "Epoch 57, Loss: 28.495567321777344\n",
      "Epoch 58, Loss: 27.87052976168119\n",
      "Epoch 59, Loss: 27.29912625826322\n",
      "Epoch 60, Loss: 28.137763830331657\n",
      "Epoch 61, Loss: 26.63881668677697\n",
      "Epoch 62, Loss: 26.967086498553936\n",
      "Epoch 63, Loss: 27.34585820711576\n",
      "Epoch 64, Loss: 27.25659282390888\n",
      "Epoch 65, Loss: 27.062783901508038\n",
      "Epoch 66, Loss: 27.398803564218376\n",
      "Epoch 67, Loss: 27.644366924579327\n",
      "Epoch 68, Loss: 27.441344774686375\n",
      "Epoch 69, Loss: 26.760898736807015\n",
      "Epoch 70, Loss: 27.075180934025692\n",
      "Epoch 71, Loss: 27.027920796320988\n",
      "Epoch 72, Loss: 27.13821631211501\n",
      "Epoch 73, Loss: 27.10338196387658\n",
      "Epoch 74, Loss: 26.456583903386043\n",
      "Epoch 75, Loss: 26.65094258235051\n",
      "Epoch 76, Loss: 26.698272705078125\n",
      "Epoch 77, Loss: 26.327300438514122\n",
      "Epoch 78, Loss: 26.265476373525765\n",
      "Epoch 79, Loss: 26.1953245309683\n",
      "Epoch 80, Loss: 25.53631855891301\n",
      "Epoch 81, Loss: 26.308372497558594\n",
      "Epoch 82, Loss: 26.35127962552584\n",
      "Epoch 83, Loss: 25.48267701955942\n",
      "Epoch 84, Loss: 26.058270381047176\n",
      "Epoch 85, Loss: 26.425332729633038\n",
      "Epoch 86, Loss: 26.116030619694637\n",
      "Epoch 87, Loss: 26.515313955453728\n",
      "Epoch 88, Loss: 25.633355360764725\n",
      "Epoch 89, Loss: 26.870007588313175\n",
      "Epoch 90, Loss: 26.16311087975135\n",
      "Epoch 91, Loss: 26.269577906681942\n",
      "Epoch 92, Loss: 26.28362303513747\n",
      "Epoch 93, Loss: 25.78367497370793\n",
      "Epoch 94, Loss: 26.201084870558518\n",
      "Epoch 95, Loss: 25.146869365985577\n",
      "Epoch 96, Loss: 26.15242312504695\n",
      "Epoch 97, Loss: 26.138458838829628\n",
      "Epoch 98, Loss: 25.553014315091648\n",
      "Epoch 99, Loss: 25.394960403442383\n",
      "Epoch 100, Loss: 25.943309490497295\n"
     ]
    }
   ],
   "source": [
    "# Define the VAE architecture for SMILES\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size * 2)  # *2 for mean and log-variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu = mu_logvar[:, :self.latent_size]\n",
    "        logvar = mu_logvar[:, self.latent_size:]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Define training function\n",
    "def train_vae(model, dataloader, optimizer, loss_function, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for smiles in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = smiles_to_fingerprint(smiles)\n",
    "            inputs = inputs.to(device)\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon_batch, inputs, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Define loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Function to convert SMILES to Morgan fingerprint\n",
    "def smiles_to_fingerprint(smiles_list, max_length=100):\n",
    "    fingerprint_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            fingerprint_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_fingerprint = nn.utils.rnn.pad_sequence(fingerprint_list, batch_first=True, padding_value=0)\n",
    "    return padded_fingerprint\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "# Define hyperparameters\n",
    "input_size = 1024  # Size of input fingerprint vector\n",
    "hidden_size = 256  # Size of hidden layer\n",
    "latent_size = 64   # Size of latent space\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize VAE model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae_model = VAE(input_size, hidden_size, latent_size).to(device)\n",
    "\n",
    "# Define optimizer with RMSprop\n",
    "optimizer = optim.RMSprop(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the VAE\n",
    "train_vae(vae_model, dataloader, optimizer, loss_function, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(vae_model.state_dict(), 'vae_model_rmsprop.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6628464",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e571f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae_model is your trained VAE model\n",
    "# Save only the model weights to a file\n",
    "torch.save(vae_model.state_dict(), 'vae_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff0eba",
   "metadata": {},
   "source": [
    "## Generating new molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8d6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule 1: tensor([[ 0.0622, -0.0788,  0.0451,  ...,  0.1157, -0.0652, -0.0735]])\n",
      "Molecule 2: tensor([[ 0.1505, -0.0474,  0.0889,  ..., -0.0147,  0.0341, -0.1232]])\n",
      "Molecule 3: tensor([[-0.0211, -0.1066,  0.0700,  ...,  0.0662, -0.0384,  0.0293]])\n",
      "Molecule 4: tensor([[-0.0493,  0.0044,  0.0548,  ...,  0.0685, -0.0520,  0.0157]])\n",
      "Molecule 5: tensor([[-0.1191, -0.0528,  0.0397,  ...,  0.2404,  0.0379, -0.1637]])\n",
      "Molecule 6: tensor([[-0.0167,  0.1652, -0.0038,  ..., -0.0151, -0.1328,  0.0911]])\n",
      "Molecule 7: tensor([[-0.0239,  0.3416,  0.0817,  ...,  0.0559,  0.0324, -0.0798]])\n",
      "Molecule 8: tensor([[-0.1857,  0.0886,  0.0935,  ..., -0.1338, -0.0724, -0.0186]])\n",
      "Molecule 9: tensor([[ 0.0240,  0.2164,  0.1047,  ...,  0.0025,  0.1231, -0.0603]])\n",
      "Molecule 10: tensor([[ 0.0570,  0.2507, -0.0237,  ..., -0.0241, -0.0310,  0.0646]])\n",
      "Molecule 11: tensor([[-0.0891,  0.1776, -0.0454,  ...,  0.0529, -0.0412, -0.0484]])\n",
      "Molecule 12: tensor([[ 0.0397,  0.0921,  0.2142,  ...,  0.1068,  0.0847, -0.1989]])\n",
      "Molecule 13: tensor([[-0.0612,  0.1501,  0.0508,  ...,  0.0894, -0.0425,  0.0171]])\n",
      "Molecule 14: tensor([[-0.0552,  0.1625,  0.0295,  ...,  0.0498, -0.0794,  0.1424]])\n",
      "Molecule 15: tensor([[-0.0253,  0.1768, -0.0330,  ...,  0.0173, -0.0987, -0.0646]])\n",
      "Molecule 16: tensor([[ 0.0429,  0.0909, -0.0626,  ...,  0.1948,  0.0540, -0.0858]])\n",
      "Molecule 17: tensor([[ 0.1223,  0.1154, -0.2351,  ...,  0.0594,  0.0204, -0.0996]])\n",
      "Molecule 18: tensor([[ 0.0707,  0.1390,  0.1517,  ...,  0.1118, -0.0552,  0.0107]])\n",
      "Molecule 19: tensor([[-0.0535,  0.0482,  0.1659,  ...,  0.0187,  0.0828,  0.0524]])\n",
      "Molecule 20: tensor([[ 0.0427, -0.0883,  0.1130,  ...,  0.0581, -0.1872, -0.0508]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vae_model.load_state_dict(torch.load('vae_model_weights.pth'))  # Load the trained model weights\n",
    "vae_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the number of molecules to generate\n",
    "num_molecules = 20\n",
    "\n",
    "# Generate new molecules\n",
    "generated_molecules = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_molecules):\n",
    "        # Sample from the latent space (e.g., Gaussian distribution)\n",
    "        latent_sample = torch.randn(1, vae_model.latent_size)\n",
    "\n",
    "        # Decode the latent sample to generate a molecular structure\n",
    "        decoded_molecule = vae_model.decoder(latent_sample)\n",
    "\n",
    "        # Add the generated molecule to the list\n",
    "        generated_molecules.append(decoded_molecule)\n",
    "\n",
    "# Print the generated molecules\n",
    "for idx, molecule in enumerate(generated_molecules):\n",
    "    print(f\"Molecule {idx + 1}: {molecule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5cb80",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f52513-6184-44fb-bd70-1d89256876d5",
   "metadata": {},
   "source": [
    "### 1. Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea822b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Generator Loss: 0.6855122745037079, Discriminator Loss: 1.3769463300704956\n",
      "Epoch 2, Generator Loss: 0.6848061978816986, Discriminator Loss: 1.3486742973327637\n",
      "Epoch 3, Generator Loss: 0.683135986328125, Discriminator Loss: 1.3228106498718262\n",
      "Epoch 4, Generator Loss: 0.679880291223526, Discriminator Loss: 1.2991365790367126\n",
      "Epoch 5, Generator Loss: 0.6771744787693024, Discriminator Loss: 1.2706681489944458\n",
      "Epoch 6, Generator Loss: 0.6709451973438263, Discriminator Loss: 1.2490885853767395\n",
      "Epoch 7, Generator Loss: 0.6650155186653137, Discriminator Loss: 1.2289321422576904\n",
      "Epoch 8, Generator Loss: 0.6576692163944244, Discriminator Loss: 1.209460437297821\n",
      "Epoch 9, Generator Loss: 0.6532025337219238, Discriminator Loss: 1.194105625152588\n",
      "Epoch 10, Generator Loss: 0.6499802768230438, Discriminator Loss: 1.1828950643539429\n",
      "Epoch 11, Generator Loss: 0.644739955663681, Discriminator Loss: 1.1910693049430847\n",
      "Epoch 12, Generator Loss: 0.6535230875015259, Discriminator Loss: 1.1837641596794128\n",
      "Epoch 13, Generator Loss: 0.6728312969207764, Discriminator Loss: 1.1723570823669434\n",
      "Epoch 14, Generator Loss: 0.7106402516365051, Discriminator Loss: 1.1478681564331055\n",
      "Epoch 15, Generator Loss: 0.7574337720870972, Discriminator Loss: 1.1202539205551147\n",
      "Epoch 16, Generator Loss: 0.8214328289031982, Discriminator Loss: 1.0715447664260864\n",
      "Epoch 17, Generator Loss: 0.8916848599910736, Discriminator Loss: 1.0241808891296387\n",
      "Epoch 18, Generator Loss: 0.9677976071834564, Discriminator Loss: 0.9870995581150055\n",
      "Epoch 19, Generator Loss: 1.0655889511108398, Discriminator Loss: 0.949190616607666\n",
      "Epoch 20, Generator Loss: 1.1430633068084717, Discriminator Loss: 0.9355395436286926\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Define a custom dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list):\n",
    "        self.smiles_list = smiles_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles_list[idx]\n",
    "\n",
    "# Define the Generator architecture for SMILES\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator architecture for SMILES\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define training function for GAN\n",
    "def train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=10):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        g_running_loss = 0.0\n",
    "        d_running_loss = 0.0\n",
    "        for smiles in dataloader:\n",
    "            real_smiles = smiles_to_tensor(smiles).to(device)\n",
    "            \n",
    "            # Train discriminator with real data\n",
    "            d_optimizer.zero_grad()\n",
    "            real_output = discriminator(real_smiles)\n",
    "            real_labels = torch.ones(real_smiles.size(0), 1).to(device)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Train discriminator with fake data\n",
    "            latent_samples = torch.randn(real_smiles.size(0), latent_size).to(device)\n",
    "            fake_smiles = generator(latent_samples)\n",
    "            fake_output = discriminator(fake_smiles.detach())\n",
    "            fake_labels = torch.zeros(real_smiles.size(0), 1).to(device)\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Update discriminator weights\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            d_running_loss += d_loss.item()\n",
    "            \n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_output = discriminator(fake_smiles)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            g_running_loss += g_loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Generator Loss: {g_running_loss / len(dataloader)}, Discriminator Loss: {d_running_loss / len(dataloader)}')\n",
    "\n",
    "# Function to convert SMILES to tensor\n",
    "def smiles_to_tensor(smiles_list, max_length=100):\n",
    "    tensor_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            tensor_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_tensor = nn.utils.rnn.pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    return padded_tensor\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "latent_size = 100\n",
    "output_size = 1024\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(latent_size, output_size).to(device)\n",
    "discriminator = Discriminator(output_size).to(device)\n",
    "\n",
    "# Define optimizers and criterion\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained generator weights\n",
    "torch.save(generator.state_dict(), 'generator_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26571d21-427b-4ce2-9f63-66c7ecaa82a1",
   "metadata": {},
   "source": [
    "### 2. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f3b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Generator Loss: 0.6812567114830017, Discriminator Loss: 1.3893267512321472\n",
      "Epoch 2, Generator Loss: 0.6814540326595306, Discriminator Loss: 1.3888244032859802\n",
      "Epoch 3, Generator Loss: 0.6809656620025635, Discriminator Loss: 1.3897905945777893\n",
      "Epoch 4, Generator Loss: 0.6811710298061371, Discriminator Loss: 1.3892515897750854\n",
      "Epoch 5, Generator Loss: 0.6810757219791412, Discriminator Loss: 1.389388084411621\n",
      "Epoch 6, Generator Loss: 0.6817755699157715, Discriminator Loss: 1.3887900114059448\n",
      "Epoch 7, Generator Loss: 0.68106409907341, Discriminator Loss: 1.3892813920974731\n",
      "Epoch 8, Generator Loss: 0.6811458170413971, Discriminator Loss: 1.3890327215194702\n",
      "Epoch 9, Generator Loss: 0.680977463722229, Discriminator Loss: 1.38931143283844\n",
      "Epoch 10, Generator Loss: 0.6811736822128296, Discriminator Loss: 1.3889999985694885\n",
      "Epoch 11, Generator Loss: 0.6814141571521759, Discriminator Loss: 1.3887261748313904\n",
      "Epoch 12, Generator Loss: 0.6813799440860748, Discriminator Loss: 1.388796329498291\n",
      "Epoch 13, Generator Loss: 0.6814983785152435, Discriminator Loss: 1.3882755041122437\n",
      "Epoch 14, Generator Loss: 0.6813116371631622, Discriminator Loss: 1.3887994289398193\n",
      "Epoch 15, Generator Loss: 0.681416779756546, Discriminator Loss: 1.388532817363739\n",
      "Epoch 16, Generator Loss: 0.6812865436077118, Discriminator Loss: 1.3884564638137817\n",
      "Epoch 17, Generator Loss: 0.6810780465602875, Discriminator Loss: 1.3888336420059204\n",
      "Epoch 18, Generator Loss: 0.681336522102356, Discriminator Loss: 1.3886191844940186\n",
      "Epoch 19, Generator Loss: 0.6816319525241852, Discriminator Loss: 1.387911081314087\n",
      "Epoch 20, Generator Loss: 0.6813929975032806, Discriminator Loss: 1.3882842659950256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Define a custom dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list):\n",
    "        self.smiles_list = smiles_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles_list[idx]\n",
    "\n",
    "# Define the Generator architecture for SMILES\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator architecture for SMILES\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define training function for GAN\n",
    "def train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=10):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        g_running_loss = 0.0\n",
    "        d_running_loss = 0.0\n",
    "        for smiles in dataloader:\n",
    "            real_smiles = smiles_to_tensor(smiles).to(device)\n",
    "            \n",
    "            # Train discriminator with real data\n",
    "            d_optimizer.zero_grad()\n",
    "            real_output = discriminator(real_smiles)\n",
    "            real_labels = torch.ones(real_smiles.size(0), 1).to(device)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Train discriminator with fake data\n",
    "            latent_samples = torch.randn(real_smiles.size(0), latent_size).to(device)\n",
    "            fake_smiles = generator(latent_samples)\n",
    "            fake_output = discriminator(fake_smiles.detach())\n",
    "            fake_labels = torch.zeros(real_smiles.size(0), 1).to(device)\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Update discriminator weights\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            d_running_loss += d_loss.item()\n",
    "            \n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_output = discriminator(fake_smiles)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            g_running_loss += g_loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Generator Loss: {g_running_loss / len(dataloader)}, Discriminator Loss: {d_running_loss / len(dataloader)}')\n",
    "\n",
    "# Function to convert SMILES to tensor\n",
    "def smiles_to_tensor(smiles_list, max_length=100):\n",
    "    tensor_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            tensor_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_tensor = nn.utils.rnn.pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    return padded_tensor\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "latent_size = 100\n",
    "output_size = 1024\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(latent_size, output_size).to(device)\n",
    "discriminator = Discriminator(output_size).to(device)\n",
    "\n",
    "# Define optimizers and criterion\n",
    "g_optimizer = optim.SGD(generator.parameters(), lr=learning_rate)\n",
    "d_optimizer = optim.SGD(discriminator.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained generator weights\n",
    "torch.save(generator.state_dict(), 'generator_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee1d78-f77b-40d1-bf54-1fc27c791aa8",
   "metadata": {},
   "source": [
    "### 3. Root Mean Squared Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d218df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Generator Loss: 0.7479954361915588, Discriminator Loss: 1.3523942828178406\n",
      "Epoch 2, Generator Loss: 0.7173742353916168, Discriminator Loss: 1.1401679515838623\n",
      "Epoch 3, Generator Loss: 0.6339037716388702, Discriminator Loss: 1.5157968997955322\n",
      "Epoch 4, Generator Loss: 0.7592118978500366, Discriminator Loss: 1.4662600755691528\n",
      "Epoch 5, Generator Loss: 0.7702058255672455, Discriminator Loss: 1.3066260814666748\n",
      "Epoch 6, Generator Loss: 0.7477568089962006, Discriminator Loss: 1.332560956478119\n",
      "Epoch 7, Generator Loss: 0.7205433249473572, Discriminator Loss: 1.3181023001670837\n",
      "Epoch 8, Generator Loss: 0.739948034286499, Discriminator Loss: 1.293932318687439\n",
      "Epoch 9, Generator Loss: 0.7708102166652679, Discriminator Loss: 1.2207165360450745\n",
      "Epoch 10, Generator Loss: 0.7617858946323395, Discriminator Loss: 1.20681893825531\n",
      "Epoch 11, Generator Loss: 0.7763881981372833, Discriminator Loss: 1.165480613708496\n",
      "Epoch 12, Generator Loss: 0.8110237419605255, Discriminator Loss: 1.0569046139717102\n",
      "Epoch 13, Generator Loss: 0.8078569769859314, Discriminator Loss: 1.0159058570861816\n",
      "Epoch 14, Generator Loss: 0.8070226907730103, Discriminator Loss: 1.004611074924469\n",
      "Epoch 15, Generator Loss: 0.832566887140274, Discriminator Loss: 0.9787859916687012\n",
      "Epoch 16, Generator Loss: 0.8528968989849091, Discriminator Loss: 0.9377486407756805\n",
      "Epoch 17, Generator Loss: 0.8673447668552399, Discriminator Loss: 0.9098845720291138\n",
      "Epoch 18, Generator Loss: 0.8977636992931366, Discriminator Loss: 0.9117458462715149\n",
      "Epoch 19, Generator Loss: 0.9213769435882568, Discriminator Loss: 0.9299202263355255\n",
      "Epoch 20, Generator Loss: 0.9928374886512756, Discriminator Loss: 0.983767181634903\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Define a custom dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list):\n",
    "        self.smiles_list = smiles_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles_list[idx]\n",
    "\n",
    "# Define the Generator architecture for SMILES\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator architecture for SMILES\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define training function for GAN\n",
    "def train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=10):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        g_running_loss = 0.0\n",
    "        d_running_loss = 0.0\n",
    "        for smiles in dataloader:\n",
    "            real_smiles = smiles_to_tensor(smiles).to(device)\n",
    "            \n",
    "            # Train discriminator with real data\n",
    "            d_optimizer.zero_grad()\n",
    "            real_output = discriminator(real_smiles)\n",
    "            real_labels = torch.ones(real_smiles.size(0), 1).to(device)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Train discriminator with fake data\n",
    "            latent_samples = torch.randn(real_smiles.size(0), latent_size).to(device)\n",
    "            fake_smiles = generator(latent_samples)\n",
    "            fake_output = discriminator(fake_smiles.detach())\n",
    "            fake_labels = torch.zeros(real_smiles.size(0), 1).to(device)\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Update discriminator weights\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            d_running_loss += d_loss.item()\n",
    "            \n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_output = discriminator(fake_smiles)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            g_running_loss += g_loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Generator Loss: {g_running_loss / len(dataloader)}, Discriminator Loss: {d_running_loss / len(dataloader)}')\n",
    "\n",
    "# Function to convert SMILES to tensor\n",
    "def smiles_to_tensor(smiles_list, max_length=100):\n",
    "    tensor_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "            fingerprint = np.array(fingerprint).astype(float)\n",
    "            tensor_list.append(torch.from_numpy(fingerprint).float())\n",
    "    padded_tensor = nn.utils.rnn.pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    return padded_tensor\n",
    "\n",
    "# Example SMILES data\n",
    "smiles_data = df1['SMILES']\n",
    "latent_size = 100\n",
    "output_size = 1024\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SMILESDataset(smiles_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(latent_size, output_size).to(device)\n",
    "discriminator = Discriminator(output_size).to(device)\n",
    "\n",
    "# Define optimizers and criterion\n",
    "g_optimizer = optim.RMSprop(generator.parameters(), lr=learning_rate)\n",
    "d_optimizer = optim.RMSprop(discriminator.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, dataloader, g_optimizer, d_optimizer, criterion, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained generator weights\n",
    "torch.save(generator.state_dict(), 'generator_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
